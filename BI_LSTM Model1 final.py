import pandas as pd
import numpy as np
import datetime
import tensorflow as tf
from tensorflow.keras.layers import Input, Bidirectional, LSTM, Dropout, Dense, LayerNormalization, Add, Flatten, Attention
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback
from tensorflow.keras.regularizers import l2
from sklearn.model_selection import StratifiedShuffleSplit, RandomizedSearchCV
from scikeras.wrappers import KerasClassifier
from sklearn.metrics import accuracy_score, mean_squared_error, f1_score
from sklearn.preprocessing import StandardScaler
import gc
import matplotlib.pyplot as plt


# Load necessary columns and define chunk size
necessary_columns = ['lat', 'lon', 'alt', 'time', 'label', 'tid', 'time_diff', 'partition', 'distance', 'speed', 'acceleration', 'bearing', 'pitch', 'user']
chunksize = 10000  # Reduced chunk size to manage memory better


# Function to process chunk
def process_chunk(chunk):

    """
    Process a chunk of data to extract time features and other necessary features,
    and convert them to float32 to save memory.

    Args:
    - chunk (DataFrame): Chunk of data to process.

    Returns:
    - np.array: Processed feature data.
    - np.array: Processed labels.
    """
    
    # Convert time to features
    time_features = []

    # Iterate over each row in the chunk to extract time-related features
    for _, row in chunk.iterrows():
        # Convert the 'time' string into a datetime object
        timestamp = datetime.datetime.strptime(str(row['time']), '%Y-%m-%d %H:%M:%S')

        # Append extracted time features: hour, minute, second, and weekday
        time_features.append([
            timestamp.hour,        # Hour of the timestamp
            timestamp.minute,      # Minute of the timestamp
            timestamp.second,      # Second of the timestamp
            timestamp.weekday()    # Day of the week (0=Monday, ..., 6=Sunday)
        ])

    # Convert the list of time features into a NumPy array    
    time_features = np.array(time_features)

    # Extract necessary features and convert to float32 to save memory
    other_features = chunk[['lat', 'lon', 'alt', 'distance', 'speed', 'acceleration', 'bearing', 'pitch']].values.astype(np.float32)

    # Combine features and labels
    X_chunk = np.concatenate((other_features, time_features), axis=1)
    y_chunk = chunk[['label', 'user']].values.astype(np.float32)

    return X_chunk, y_chunk


# Function to reshape data for LSTM input
def reshape_data(X, y, sequence_length=10):

    """
    Reshape data into sequences for LSTM input.
    
    Parameters:
    X (array-like): Feature data.
    y (array-like): Labels corresponding to the data.
    sequence_length (int): The length of sequences to be created.

    Returns:
    np.array: Reshaped feature data in sequences.
    np.array: Corresponding labels for the sequences.

    """
    Xs, ys = [], []    # Initialize empty lists to hold the sequences and corresponding labels

     # Loop through the data, creating sequences of the specified length
     for i in range(len(X) - sequence_length):
        # Append a sequence of features to the Xs list
        Xs.append(X[i:(i + sequence_length)])
        # Append the corresponding label to the ys list (label is at the end of the sequence)
        ys.append(y[i + sequence_length])

    # Convert lists to NumPy arrays for efficient numerical computations    
    return np.array(Xs), np.array(ys)


# Load and process training data in chunks
X_train_chunks = []
y_train_chunks = []
file_path = '/content/gdrive/MyDrive/subset_data5.csv'  # Use the correct path to your data file


# Read the entire dataset for stratified splitting
data = pd.read_csv(file_path, usecols=necessary_columns)
X_data, y_data = process_chunk(data)


# Free up memory
del data
gc.collect()  # Force garbage collection


# Normalize the features
scaler = StandardScaler()
X_data = scaler.fit_transform(X_data)


# Use StratifiedShuffleSplit to split data into training (70%) and testing sets (30%)
sss = StratifiedShuffleSplit(n_splits=1, test_size=0.3, random_state=42)

# Iterate over the splits generated by StratifiedShuffleSplit
for train_index, test_index in sss.split(X_data, y_data[:, 0]):
    # Split based on the 'label' column to maintain class distribution
    X_train, X_test = X_data[train_index], X_data[test_index]     # Split features into training and testing sets
    y_train, y_test = y_data[train_index], y_data[test_index]     # Split labels into training and testing sets


# Reshape data for LSTM input
sequence_length = 10
X_train, y_train = reshape_data(X_train, y_train, sequence_length)   # Reshape training data into sequences of length sequence_lengt
X_test, y_test = reshape_data(X_test, y_test, sequence_length)       # Reshape testing data into sequences of length sequence_length

# One-hot encode labels for multi-class classification
num_classes = len(np.unique(y_train[:, 0]))   # Determine the number of classes in the training labels
y_train_labels = tf.keras.utils.to_categorical(y_train[:, 0], num_classes=num_classes)   # Convert training labels to one-hot encoding
y_test_labels = tf.keras.utils.to_categorical(y_test[:, 0], num_classes=num_classes)     # Convert testing labels to one-hot encoding

# Define a function to create the Keras model with L2 regularization
def create_bilstm_attention_model(lstm_units_1=256, lstm_units_2=128, lstm_units_3=64, dropout_rate=0.2, learning_rate=0.001, l2_reg=0.01):

    """
    Create a Bidirectional LSTM model with attention mechanism and L2 regularization.

    Args:
    - lstm_units_1 (int): Number of units/neurons in the first LSTM layer.
    - lstm_units_2 (int): Number of units/neurons in the second LSTM layer.
    - lstm_units_3 (int): Number of units/neurons in the third LSTM layer.
    - dropout_rate (float): Dropout rate for dropout layers.
    - learning_rate (float): Learning rate for the Adam optimizer.
    - l2_reg (float): L2 regularization parameter.

    Returns:
    - tf.keras.Model: Compiled Keras model.

    """
    
    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))   # Define input shape based on training data shape
    regularizer = l2(l2_reg)    # Define L2 regularization

    # First Bidirectional LSTM layer
    lstm_out = Bidirectional(LSTM(lstm_units_1, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizer))(inputs)
    # Second Bidirectional LSTM layer
    lstm_out = Bidirectional(LSTM(lstm_units_2, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizer))(lstm_out)
    # Third Bidirectional LSTM layer
    lstm_out = Bidirectional(LSTM(lstm_units_3, return_sequences=True, dropout=dropout_rate, recurrent_dropout=dropout_rate, kernel_regularizer=regularizer))(lstm_out)


    # Attention mechanism
    attention = Attention()([lstm_out, lstm_out])   # Apply attention mechanism to LSTM output
    lstm_out = Add()([lstm_out, attention])         # Add attention output to LSTM output



    # Layer normalization and flattening
    norm_out = LayerNormalization()(lstm_out)   # Normalize the output
    flatten_out = Flatten()(norm_out)     # Flatten the output
    dropout = Dropout(dropout_rate)(flatten_out)   # Apply dropout


    # Dense layers with dropout
    dense_out = Dense(512, activation='relu', kernel_regularizer=regularizer)(dropout)
    dense_out = Dropout(dropout_rate)(dense_out)   # Apply dropout
    dense_out = Dense(256, activation='relu', kernel_regularizer=regularizer)(dense_out)
    dense_out = Dropout(dropout_rate)(dense_out)    # Apply dropout

    # Output layer
    outputs = Dense(num_classes, activation='softmax')(dense_out)  # Output layer with softmax activation for multi-class classification

    model = Model(inputs, outputs)   # Create Keras model with defined inputs and outputs

    # Compile the model
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])

    return model

# Create the KerasClassifier object
model = KerasClassifier(model=create_bilstm_attention_model,
                        lstm_units_1=256,
                        lstm_units_2=128,
                        lstm_units_3=64,
                        dropout_rate=0.2,
                        learning_rate=0.001,
                        l2_reg=0.01,
                        verbose=2)

# Define the hyperparameter grid
param_dist = {
    'batch_size': [64, 128, 256],
    'epochs': [2, 3, 5],
    'lstm_units_1': [128, 256],
    'lstm_units_2': [64, 128],
    'lstm_units_3': [32, 64],
    'dropout_rate': [0.2, 0.3, 0.4],
    'learning_rate': [0.001, 0.0005, 0.0001],
    'l2_reg': [0.01, 0.001, 0.0001],
}

# Early stopping and learning rate reduction
early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)
reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)
callbacks = [early_stopping, reduce_lr]

# Create the RandomizedSearchCV object
random_search = RandomizedSearchCV(estimator=model,
                                   param_distributions=param_dist,
                                   n_iter=1,
                                   scoring='accuracy',
                                   cv=2,
                                   verbose=2,
                                   n_jobs=1)

# Fit the RandomizedSearchCV object
random_search_result = random_search.fit(X_train, y_train_labels, callbacks=callbacks, validation_data=(X_test, y_test_labels))

# Print the best parameters and best score
print("Best: %f using %s" % (random_search_result.best_score_, random_search_result.best_params_))

# Get the best model
best_model = random_search_result.best_estimator_

# Make predictions
y_pred = best_model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_test_classes = np.argmax(y_test_labels, axis=1)

# Evaluate model
accuracy = accuracy_score(y_test_classes, y_pred_classes)
mse = mean_squared_error(y_test_classes, y_pred_classes)
f1 = f1_score(y_test_classes, y_pred_classes, average='weighted')

print(f"Accuracy: {accuracy}")
print(f"MSE: {mse}")
print(f"F1 Score: {f1}")

# Plot training history
history = best_model.history_

plt.figure(figsize=(8, 4))
plt.plot(history['accuracy'], label='Training Accuracy')
plt.plot(history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure(figsize=(8, 4))
plt.plot(history['loss'], label='Training Loss')
plt.plot(history['val_loss'], label='Validation Loss')
plt.title('Loss Over Epochs')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
